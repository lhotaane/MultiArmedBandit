{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEUR: K-armed bandit problem: theory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In probability theory and machine learning, multi-armed bandit problem reffers to a model which can be seen as a set of real distributions $\\{F_1,\\dots,F_K\\}$, each distribution being associated with the rewards delivered by one of the $K\\in\\N^+$ levers. Let $\\mu_1,\\dots,\\mu_K$ be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. Several strategies or algorithms have been proposed as a solution to this problem in the last two decades. In this project, we will focus on the comparison of basic strategies in order to find (heuristically) best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define\n",
    "* $\\mathtt{episode}$ as the number of games,\n",
    "* horizon $\\mathtt{iters}$ as the number of rounds to be played in each game,\n",
    "* maximum mean reward as $\\mu^* = \\mathtt{max}\\{\\mu_1,\\mu_2\\,... \\mu_K\\},$\n",
    "* regret $\\rho$ after $T$ rounds played as $\\rho(T) = T\\mu^* - \\sum_{t=1}^{T}r_t,$ where $r_t$ is the reward in round $t$ ($\\rho$ is also a random variable),\n",
    "* expected regret as $\\mathbb{E}[\\rho(T)]$,\n",
    "* letter $\\mathtt{k}$ for the levers (arms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One way to approach this is to select each one in turn and keep track of how much you received, then keep going back to the one that paid out the most. This is possible, but, as stated before, each bandit has an underlying probability distribution associated with it, meaning that you may need more samples before finding the right one. But, each pull you spend trying to figure out the best bandit to play takes you away from maximizing your reward. This basic balancing act is known as the explore-exploit dilemma.\" - *Christian Hubbs, TowardDataScience [2]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"multi_armed_bandit.webm\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startegies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of following vizualisation, lets set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of arms\n",
    "k = 10\n",
    "# Number of rounds to be played within each game\n",
    "iters = 1000\n",
    "# Number of games to be played\n",
    "episodes = 1000\n",
    "# Mean values of arms ('random', 'sequence', 'sequence2', list, array)\n",
    "mu_input = 'sequence'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import eps_greedy_bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\varepsilon$-greedy strategy is one of the most basic option for dealing with multi armed bandit problem and is widely used because of its simplicity. It is based on finding the balance between exploration and exploitation action within the algorithm. At each round the algorithm selects\n",
    "the arm with the highest empirical mean with probability $1 − \\varepsilon$, and selects a random arm with probability $\\varepsilon$. In other word, the probability $p_i$ of choosing the $i$-th arm in the next round $t+1$ is $$p_i(t+1)=\\begin{cases}&1-\\varepsilon + \\frac{\\varepsilon}{k}\\quad\\text{    if }i=\\text{argmax}_{j}\\hat{\\mu_j}(t),\\\\&\\frac{\\varepsilon}{k}\\quad\\text{otherwise},\\end{cases}$$\n",
    "where $k$ is the number of arms and $\\hat{\\mu_j}(t)$ is the empirical mean of $j$-th arm after $t$ turns.\n",
    "\n",
    "Choosing $\\varepsilon=0$ leads to an 0- greedy algorithm, in which is in the first round randomly selected an arm that is being pulled for the rest of the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose strategies\n",
    "eps0=0\n",
    "eps1=0.05\n",
    "eps2=0.10\n",
    "eps3=0.20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "eps_0_rewards = np.zeros(iters)\n",
    "eps_1_rewards = np.zeros(iters)\n",
    "eps_2_rewards = np.zeros(iters)\n",
    "eps_3_rewards = np.zeros(iters)\n",
    "eps_0_selection = np.zeros(k)\n",
    "eps_1_selection = np.zeros(k)\n",
    "eps_2_selection = np.zeros(k)\n",
    "eps_3_selection = np.zeros(k)\n",
    "eps_0_rewards_cum = np.zeros(iters)\n",
    "eps_1_rewards_cum = np.zeros(iters)\n",
    "eps_2_rewards_cum = np.zeros(iters)\n",
    "eps_3_rewards_cum = np.zeros(iters)\n",
    "\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "    # Initialize bandits\n",
    "    eps_0 = eps_greedy_bandit(k, eps0, iters, mu=mu_input)\n",
    "    eps_1 = eps_greedy_bandit(k, eps1, iters, eps_0.mu.copy())\n",
    "    eps_2 = eps_greedy_bandit(k, eps2, iters, eps_0.mu.copy())\n",
    "    eps_3 = eps_greedy_bandit(k, eps3, iters, eps_0.mu.copy())\n",
    "    \n",
    "    # Run experiments\n",
    "    eps_0.run()\n",
    "    eps_1.run()\n",
    "    eps_2.run()\n",
    "    eps_3.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    eps_0_rewards = eps_0_rewards + (\n",
    "        eps_0.reward - eps_0_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)\n",
    "    eps_2_rewards = eps_2_rewards + (\n",
    "        eps_2.reward - eps_2_rewards) / (i + 1)\n",
    "    eps_3_rewards = eps_3_rewards + (\n",
    "        eps_3.reward - eps_3_rewards) / (i + 1)\n",
    "\n",
    "    # Average actions per episode\n",
    "    eps_0_selection = eps_0_selection + (\n",
    "        eps_0.k_n - eps_0_selection) / (i + 1)\n",
    "    eps_1_selection = eps_1_selection + (\n",
    "        eps_1.k_n - eps_1_selection) / (i + 1)\n",
    "    eps_2_selection = eps_2_selection + (\n",
    "        eps_2.k_n - eps_2_selection) / (i + 1)\n",
    "    eps_3_selection = eps_3_selection + (\n",
    "        eps_3.k_n - eps_3_selection) / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average rewards    \n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(eps_0_rewards, label=\"$\\epsilon=$\"+str(eps0))\n",
    "plt.plot(eps_1_rewards, label=\"$\\epsilon=$\"+str(eps1))\n",
    "plt.plot(eps_2_rewards, label=\"$\\epsilon=$\"+str(eps2))\n",
    "plt.plot(eps_3_rewards, label=\"$\\epsilon=$\"+str(eps3))\n",
    "for i in range(k):\n",
    "    plt.hlines(eps_0.mu[i], xmin=0,\n",
    "              xmax=iters, alpha=0.05,\n",
    "              linestyle=\"--\")\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average $\\epsilon$-greedy Rewards after \" + \n",
    "     str(episodes) + \" Episodes\")\n",
    "plt.show()\n",
    "\n",
    "# Counts of actions\n",
    "bins = np.linspace(0, k-1, k)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(bins, eps_0_selection, \n",
    "        width = 0.25, color='b', \n",
    "        label=\"$\\epsilon=$\"+str(eps0))\n",
    "plt.bar(bins+0.25, eps_1_selection,\n",
    "        width=0.25, color='g', \n",
    "        label=\"$\\epsilon=$\"+str(eps1))\n",
    "plt.bar(bins+0.50, eps_2_selection, \n",
    "        width=0.25, color='r',\n",
    "        label=\"$\\epsilon=$\"+str(eps2))\n",
    "plt.bar(bins+0.75, eps_3_selection, \n",
    "        width=0.25, color='y',\n",
    "        label=\"$\\epsilon=$\"+str(eps3))\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlim([0,k])\n",
    "plt.title(\"Actions Selected by Each Algorithm\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Number of Actions Taken\")\n",
    "plt.show()\n",
    "\n",
    "opt_per = np.array([eps_0_selection, eps_1_selection,\n",
    "                   eps_2_selection, eps_3_selection]) / iters * 100\n",
    "df = pd.DataFrame(opt_per, index=['eps='+str(eps0), \n",
    "    'eps='+str(eps1), 'eps='+str(eps2), 'eps='+str(eps3)],\n",
    "                 columns=[\"a = \" + str(x) for x in range(0, k)])\n",
    "print(\"Percentage of actions selected:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-decay strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import eps_decay_bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\varepsilon$-greedy strategies have an obvious weakness in that they continue to include random noise no matter how many examples they see. The $\\varepsilon$-decay, also known as $\\varepsilon$-decreasing strategy, is one of many modifications of $\\varepsilon$-greedy strategies. This strategy reduces the probability of exploration with every step, turning constant $\\varepsilon$ into the function of number of rounds played $\\varepsilon(t)$. We define $$\\varepsilon(t)=\\frac{1}{1+\\beta t},$$ where $\\beta<1$ is used for scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_decay_rewards = np.zeros(iters)\n",
    "eps_rewards = np.zeros(iters)\n",
    "\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "    # Initialize bandits\n",
    "    eps_decay = eps_decay_bandit(k, iters, 1/k, mu=mu_input)\n",
    "    eps_1 = eps_greedy_bandit(k, 0.1, iters, eps_decay.mu.copy())\n",
    "    \n",
    "    # Run experiments\n",
    "    eps_decay.run()\n",
    "    eps_1.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    eps_decay_rewards = eps_decay_rewards + (\n",
    "        eps_decay.reward - eps_decay_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rewards\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(eps_decay_rewards, label=\"$\\epsilon-decay$\")\n",
    "plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average $\\epsilon-decay$ and\" + \n",
    "    \"$\\epsilon-greedy$ Rewards after \" \n",
    "    + str(episodes) + \" Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic Initial Value strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we leave the $\\varepsilon$ strategies for a while, the Optimistic Initial value strategy offers almost oposite approach to finding optimal strategy. The difference lies within the absence of adding random noice in order to find best action. Instead, we over estimate the rewards of all the actions and continuously select the maximum. In this case, the algorithm explores early on as it seeks to maximize its returns while additional information allows the values to converge to their true means. This approach does require some additional background knowledge to be included in the set up because we need at least some idea of what the rewards are so that we can over estimate them.\n",
    "\n",
    "Because of this definition, we can this strategy prepare from 0-greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "oiv_rewards = np.zeros(iters)\n",
    "eps_decay_rewards = np.zeros(iters)\n",
    "eps_1_rewards = np.zeros(iters)\n",
    "\n",
    "# Select initial values\n",
    "oiv_init = np.repeat(5., k)\n",
    "\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "    # Initialize bandits\n",
    "    oiv_bandit = eps_greedy_bandit(k, 0, iters, mu=mu_input)\n",
    "    oiv_bandit.k_reward = oiv_init.copy()\n",
    "    oiv_bandit.k_n = np.ones(k)\n",
    "    eps_decay = eps_decay_bandit(k, iters,1/k, oiv_bandit.mu.copy())\n",
    "    eps_1 = eps_greedy_bandit(k, 0.1, iters, oiv_bandit.mu.copy())\n",
    "    \n",
    "    # Run experiments\n",
    "    oiv_bandit.run()\n",
    "    eps_decay.run()\n",
    "    eps_1.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    oiv_rewards = oiv_rewards + (\n",
    "        oiv_bandit.reward - oiv_rewards) / (i + 1)\n",
    "    eps_decay_rewards = eps_decay_rewards + (\n",
    "        eps_decay.reward - eps_decay_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(oiv_rewards, label=\"OIV\")\n",
    "plt.plot(eps_decay_rewards, label=\"$\\epsilon-decay$\")\n",
    "plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average Bandit Strategy Rewards after \" + \n",
    "    str(episodes) + \" Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"number of selections\": oiv_bandit.k_n - 1,\n",
    "                  \"actual reward\": oiv_bandit.mu,\n",
    "                  \"estimated reward\": oiv_bandit.k_reward})\n",
    "df = df.applymap(lambda x: np.round(x, 2))\n",
    "df['number of selections'] = df['number of selections'].astype('int')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftMax strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import softmax_bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SoftMax strategy takes $\\varepsilon$ strategies to another level and pick each arm with a probability that is proportional to its average reward. Arms with greater empirical means are therefore picked with higher probability. In this project, we use Boltzmann (Gibbs) exploration $$p_i(t+1) =\\frac{e^{\\frac{\\hat{\\mu}_i}{\\tau}}}{\\sum_{j=1}^{k}e^{\\frac{\\hat{\\mu}_j}{\\tau}}},$$\n",
    "where $\\hat{\\mu}_i$ is the estimated mean of the rewards brought by the lever $i$ and $\\tau\\in\\R^+$ is a parameter called the temperature. The choice of $\\tau$’s value is left to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm_1_rewards = np.zeros(iters)\n",
    "sfm_2_rewards = np.zeros(iters)\n",
    "sfm_3_rewards = np.zeros(iters)\n",
    "sfm_4_rewards = np.zeros(iters)\n",
    "eps_1_rewards = np.zeros(iters)\n",
    "\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "    # Initialize bandits\n",
    "    sfm1 = softmax_bandit(k, iters, T=0.05, mu=mu_input)\n",
    "    sfm2 = softmax_bandit(k, iters, T=0.1, mu=sfm1.mu.copy())\n",
    "    sfm3 = softmax_bandit(k, iters, T=0.2, mu=sfm1.mu.copy())\n",
    "    sfm4 = softmax_bandit(k, iters, T=3, mu=sfm1.mu.copy())\n",
    "    eps_1 = eps_greedy_bandit(k, 0.1, iters, mu=sfm1.mu.copy())\n",
    "    \n",
    "    # Run experiments\n",
    "    sfm1.run()\n",
    "    sfm2.run()\n",
    "    sfm3.run()\n",
    "    sfm4.run()\n",
    "    eps_1.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    sfm_1_rewards = sfm_1_rewards + (\n",
    "        sfm1.reward - sfm_1_rewards) / (i + 1)\n",
    "    sfm_2_rewards = sfm_2_rewards + (\n",
    "        sfm2.reward - sfm_2_rewards) / (i + 1)\n",
    "    sfm_3_rewards = sfm_3_rewards + (\n",
    "        sfm3.reward - sfm_3_rewards) / (i + 1)\n",
    "    sfm_4_rewards = sfm_4_rewards + (\n",
    "        sfm4.reward - sfm_4_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sfm_1_rewards, label=\"SoftMax: T=\"+str(sfm1.T))\n",
    "plt.plot(sfm_2_rewards, label=\"SoftMax: T=\"+str(sfm2.T))\n",
    "plt.plot(sfm_3_rewards, label=\"SoftMax: T=\"+str(sfm3.T))\n",
    "plt.plot(sfm_4_rewards, label=\"SoftMax: T=\"+str(sfm4.T))\n",
    "plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average $\\epsilon-decay$ and\" + \n",
    "    \"$\\epsilon-greedy$ Rewards after \" \n",
    "    + str(episodes) + \" Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftMax using Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import annealing_softmax_bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to upgrade the SoftMx strategy is the use of Fast Simulated Annealing. In this approach, the constant temperature $\\tau$ is replaced by a function $\\tau(t)$ defined as $$\\tau(t) = \\frac{1}{log(t+0.0.0000001)},$$ where the constant $0.0000001$ is for avoiding $log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm_1_rewards = np.zeros(iters)\n",
    "sfm_2_rewards = np.zeros(iters)\n",
    "eps_1_rewards = np.zeros(iters)\n",
    "\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "    # Initialize bandits\n",
    "    sfm1 = annealing_softmax_bandit(k, iters, mu=mu_input, cooling='abs')\n",
    "    sfm2 = annealing_softmax_bandit(k, iters, mu=sfm1.mu.copy(), cooling='log')\n",
    "    eps_1 = annealing_softmax_bandit(k, iters, mu=sfm1.mu.copy(), cooling='random')\n",
    "    \n",
    "    # Run experiments\n",
    "    sfm1.run()\n",
    "    sfm2.run()\n",
    "    eps_1.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    sfm_1_rewards = sfm_1_rewards + (\n",
    "        sfm1.reward - sfm_1_rewards) / (i + 1)\n",
    "    sfm_2_rewards = sfm_2_rewards + (\n",
    "        sfm2.reward - sfm_2_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sfm_1_rewards, label=\"SoftMax+SA\")\n",
    "plt.plot(sfm_2_rewards, label=\"SoftMax: T=\"+str(sfm2.T))\n",
    "plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average $\\epsilon-decay$ and\" + \n",
    "    \"$\\epsilon-greedy$ Rewards after \" \n",
    "    + str(episodes) + \" Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEUR: K-armed bandit problem: comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for the best strategy for playing 1000 rounds (normalized over 1000 games)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import eps_greedy_bandit, eps_decay_bandit, softmax_bandit, annealing_softmax_bandit\n",
    "\n",
    "# Number of arms\n",
    "k = 10\n",
    "# Number of rounds to be played within each game\n",
    "iters = 1000\n",
    "# Number of games to be played\n",
    "episodes = 1000\n",
    "# Mean values of arms ('random', 'sequence', 'sequence2', list, array)\n",
    "mu_input = 'sequence'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best $\\varepsilon$-greedy strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output disctionary\n",
    "eps_greedy_results = {}\n",
    "\n",
    "for epsilon in np.linspace(0,1,101):\n",
    "    epsilon = round(epsilon,2)\n",
    "    # Initialization\n",
    "    eps_rewards = np.zeros(iters)\n",
    "    \n",
    "    for i in range(episodes):\n",
    "         # Initialize bandits\n",
    "        eps_greedy_b = eps_greedy_bandit(k, epsilon, iters, mu=mu_input)\n",
    "        # Run experiments\n",
    "        eps_greedy_b.run()\n",
    "        # Update long-term averages\n",
    "        eps_rewards = eps_rewards + (eps_greedy_b.reward - eps_rewards) / (i + 1)\n",
    "    \n",
    "    eps_greedy_results['eps='+str(epsilon)] = eps_rewards\n",
    "\n",
    "df_greedy = pd.DataFrame(data = eps_greedy_results, index=range(1000))\n",
    "\n",
    "print('Best $epsilon$-greedy strategy:',df_greedy.loc[df_greedy.shape[0]-1].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best $\\varepsilon$-decay strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_decay_results = {}\n",
    "\n",
    "for beta in np.linspace(0,1,11):\n",
    "    # Initialization\n",
    "    bandit_rewards = np.zeros(iters)\n",
    "    beta = round(beta,2)\n",
    "    for i in range(episodes):\n",
    "        bandit = eps_decay_bandit(k, iters, beta, mu=mu_input)\n",
    "        # Run experiments\n",
    "        bandit.run()\n",
    "        # Update long-term averages\n",
    "        bandit_rewards = bandit_rewards + (bandit.reward - bandit_rewards) / (i + 1)\n",
    "\n",
    "    eps_decay_results['beta='+str(beta)] = bandit_rewards \n",
    "\n",
    "df_decay = pd.DataFrame(data = eps_decay_results, index=range(1000))\n",
    "\n",
    "print('Best $\\epsilon$-decay strategy:',df_decay.loc[df_decay.shape[0]-1].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best Optimistic Value Strategy\n",
    "\n",
    "The only thing we can change here are the initial estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oiv_results = {}\n",
    "\n",
    "for est in range(0,15):\n",
    "    # Initialization\n",
    "    bandit_rewards = np.zeros(iters)\n",
    "    # Select initial values\n",
    "    oiv_init = np.repeat(est, k)\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        bandit = eps_greedy_bandit(k, 0, iters, mu=mu_input)\n",
    "        bandit.k_reward = oiv_init.copy()\n",
    "        bandit.k_n = np.ones(k)\n",
    "        # Run experiments\n",
    "        bandit.run()\n",
    "        # Update long-term averages\n",
    "        bandit_rewards = bandit_rewards + (bandit.reward - bandit_rewards) / (i + 1)\n",
    "\n",
    "    oiv_results['estimate='+str(est)] = bandit_rewards \n",
    "\n",
    "df_oiv = pd.DataFrame(data = oiv_results, index=range(1000))\n",
    "\n",
    "print('Best strategy:',df_oiv.loc[df_oiv.shape[0]-1].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best SoftMax strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_results = {}\n",
    "\n",
    "for temp in np.linspace(0.05,5,100):\n",
    "    # Initialization\n",
    "    bandit_rewards = np.zeros(iters)\n",
    "    temp = round(temp,2)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        bandit = softmax_bandit(k, iters, T=temp, mu=mu_input)\n",
    "        # Run experiments\n",
    "        bandit.run()\n",
    "        # Update long-term averages\n",
    "        bandit_rewards = bandit_rewards + (bandit.reward - bandit_rewards) / (i + 1)\n",
    "\n",
    "    softmax_results['T='+str(temp)] = bandit_rewards \n",
    "\n",
    "df_softmax = pd.DataFrame(data = softmax_results, index=range(1000))\n",
    "\n",
    "print('Best SoftMax strategy:',df_softmax.loc[df_softmax.shape[0]-1].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best SoftMax strategy using Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_softmax_results = {}\n",
    "\n",
    "for z in ['abs','log','random']:\n",
    "    # Initialization\n",
    "    bandit_rewards = np.zeros(iters)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        bandit = annealing_softmax_bandit(k, iters, mu=mu_input, cooling=z)\n",
    "        # Run experiments\n",
    "        bandit.run()\n",
    "        # Update long-term averages\n",
    "        bandit_rewards = bandit_rewards + (bandit.reward - bandit_rewards) / (i + 1)\n",
    "\n",
    "    SA_softmax_results['SA'+z] = bandit_rewards \n",
    "\n",
    "df_SA = pd.DataFrame(data = SA_softmax_results, index=range(1000))\n",
    "\n",
    "print('Best SoftMax + SA strategy:',df_SA.loc[df_SA.shape[0]-1].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(df_greedy[df_greedy.loc[df_greedy.shape[0]-1].idxmax()], label=\"Greedy: \"+df_greedy.loc[df_greedy.shape[0]-1].idxmax())\n",
    "plt.plot(df_decay[df_decay.loc[df_decay.shape[0]-1].idxmax()], label=\"Decay: \"+df_decay.loc[df_decay.shape[0]-1].idxmax())\n",
    "plt.plot(df_oiv[df_oiv.loc[df_oiv.shape[0]-1].idxmax()], label=\"OIV: \"+df_oiv.loc[df_oiv.shape[0]-1].idxmax())\n",
    "plt.plot(df_softmax[df_softmax.loc[df_softmax.shape[0]-1].idxmax()], label=\"SoftMax: \"+df_softmax.loc[df_oiv.shape[0]-1].idxmax())\n",
    "plt.plot(df_SA[df_SA.loc[df_SA.shape[0]-1].idxmax()], label=\"SA: \"+df_SA.loc[df_SA.shape[0]-1].idxmax())\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average Rewards after \" \n",
    "    + str(episodes) + \" Episodes for each strategy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94e569850c11457be8d2d1fa2381852711d2fff11a9f9786335551a3e61a4fcf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
